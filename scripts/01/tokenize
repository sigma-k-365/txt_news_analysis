# read all .txt files in the directory and get list of (file name, text)
load(paste(data_processed_path, "\\df_txt_date_news.rdata", sep=""))

# tokenize all news
filtered_tokens <- df_txt_date_news %>% 
unnest_tokens(output = word, input = news) %>%
    mutate(stem = wordStem(word, language="german")) %>%   # stems words and creates column
    filter(! grepl('[0-9]', word) ) %>%    # remove numbers
    filter(! word %in% extended_stopwords_de) # remove stopwords

# count word by month
word_count <- 
    filtered_tokens %>% 
    count(word) %>% 
    filter(n >= 50) %>% # remove minor words
    arrange(desc(n))

# count word by month
month_word_count <- 
    filtered_tokens %>% 
    group_by(rounded_by_month = lubridate::floor_date(date, 'month')) %>%
    count(rounded_by_month, word) %>% 
    filter(n >= 10) %>% # remove minor words
    arrange(desc(n))

# count occurance of "trump" per month
month_trump_count <- 
    month_word_count %>% 
    filter(word == "trump") %>% 
    arrange(rounded_by_month)
